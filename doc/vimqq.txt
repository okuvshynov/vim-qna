*vimqq.txt*  For Vim version 8.0  Last change: 2024 July 23

VIMQQ ~

1. Introduction     |vimqq-intro|
2. Installation     |vimqq-install|
3. Usage            |vimqq-usage|
4. Commands         |vimqq-commands|
5. Mappings         |vimqq-mappings|
6. Configuration    |vimqq-config|
7. Changelog        |vimqq-changelog|

==============================================================================
1. Introduction                                                    *vimqq-intro*

AI plugin with a focus on code reading, explanation, education and coaching 
rather than writing new code.

While impressive, LLMs are still not that good at writing original code. It
is especially true for the scenarios where the change is spread out across
multiple files in a huge repository, which is exactly what some very important
and time consuming code changes are.

The commonly cited rule of thumb metric is that software engineers spend 10x 
more time reading and comprehending the code rather than writing new code and
vimqq is an attempt to help with this aspect of the workflow.

What vimqq is not doing:
 - generating code in place, typing it in editor directly, all communication
   is done in the chat buffer. It is reasonably easy to copy/paste the code.

Features:
 - Claude/Anthropic remote models through paid API
 - local models via llama.cpp server (or compatible)
 - streaming response from llama.cpp server, so that user can start
   reading it as it is being generated. For example, Llama3-70B can produce 
   8-10 tokens per second on Apple M2 Ultra, which is very close to human
   reading rate. This way user will not waste any time waiting for reply.
 - KV cache warmup for llama.cpp. In cases of high-memory but low-compute
   hardware configuration for LLM inference (Apple devices, CPU-only machines)
   processing original prompt might take a while in cases of large context 
   selection or long chat history. To help with that and further amortize the 
   cost, it is possible to send and automate sending warmup queries to prefill
   the KV cache. So the workflow could look like this:
    - User selects some code in vim visual mode. 
    - User runs a command (potentially with configured hotkey). That command
      sends the incomplete message to the server and moves focus to the 
      command line, where user can start typing the question. 
    - server starts processing the query in parallel with user typing the 
      question, reducing overall wait time.
 - mixing different models in the same chat sessions. It is possible to send 
   original message to one model and use a different model for the follow-up
   questions. This allows to:
    - get multiple prospectives and opportunity to fix errors
    - save on API calls if important
    - pick the right bot based on time/complexity of the issue
    - easily fallback to more expensive/slower model if the cheaper or faster
      one was not able to give an answer.

==============================================================================
2. Installation                                                  *vimqq-install*

vimqq uses |packages| for installation.

Copy over the plugin itself:
>
    git clone https://github.com/okuvshynov/vimqq.git ~/.vim/pack/plugins/start/vimqq

The command above makes vimqq automatically loaded at vim start

Update helptags in vim:
>
    :helptags ~/.vim/pack/plugins/start/vimqq/doc


vimqq will not work in 'compatible' mode. 'nocompatible' needs to be set.

To use local models, get and build llama.cpp server
>
    git clone https://github.com/ggerganov/llama.cpp
    cd llama.cpp
    make -j 16

Download/prepare the models and start llama server:
>
    ./llama.cpp/llama-server
      --model path/to/model.gguf
      --chat-template llama3
      --host 0.0.0.0
      --port 8080

Add a bot endpoint configuration to vimrc file, for example
>
    let g:vqq_llama_servers = [
          \  {'bot_name': 'llama', 'addr': 'http://localhost:8088'},
    \]

It is possible to have multiple bots with different names.

To use claude models, register and get API key. By default vimqq will look for 
API key in environment variable `$ANTHROPIC_API_KEY`. It is possible to 
override the default with 
>
    let g:vqq_claude_api_key = ...

Bot definition looks similar to local llama.cpp:
>
    let g:vqq_claude_models = [
          \  {'bot_name': 'sonnet', 'model': 'claude-3-5-sonnet-20240620'}
    \]

==============================================================================
3. Usage                                                           *vimqq-usage*  

Assuming we have a bot named "bot", we can start with asking a question:
>
    :VQQSend @bot What are basics of unix philosophy?

We can omit the tag as well, and the first configured bot will be used:
>
    :VQQSend What are basics of unix philosophy?

After running this command new window should open in vertical split showing
the message sent and reply. If the "bot" is local llama bot, we should see
the reply being appended token by token.

Pressing "q" in the window would change the window view to the list of 
past messages. Up/Down or j/k can be used to navigate the list and <cr>
can be used to select individual chat.

The chat titles are generated by the same model as was used to produce
first reply in the chat thread.

==============================================================================
4. Commands                                                     *vimqq-commands*  

First group of commands can be used to send messages.

    - `:VQQSend [@bot] message` sends a message to current chat.
      It @bot tag is present, `@bot` will be used to generate response,
      otherwise `g:vqq_default_bot` is used. Only the message itself is sent.

    - `:VQQSendNew [@bot] message` sends a message to a new chat. 
      It @bot tag is present, `@bot` will be used to generate response,
      otherwise `g:vqq_default_bot` is used. Only the message itself is sent.

    - `:VQQSendCtx [@bot] message` sends a message to current chat.
      It @bot tag is present, `@bot` will be used to generate response,
      otherwise `g:vqq_default_bot` is used. Both message and the current 
      visual selection is sent.

    - `:VQQSendNewCtx [@bot] message` sends a message to a new chat. 
      It @bot tag is present, `@bot` will be used to generate response,
      otherwise `g:vqq_default_bot` is used. Both message and the current 
      visual selection is sent.

    the following prompt template is used for the messages with context:

>
    "Here's a code snippet:\n\n{vqq_ctx}\n\n{vqq_msg}"

    It can be modified by setting `g:vqq_context_template` variable.

Second group of commands is used for sending warmup queries.

    - `:VQQWarm [@bot]` will send current chat messages as a warmup query. 
      It @bot tag is present, `@bot` will be used to generate response,
      otherwise `g:vqq_default_bot` is used.

    - `:VQQWarmNew [@bot]` will send new chat as a warmup query. 
      It @bot tag is present, `@bot` will be used to generate response,
      otherwise `g:vqq_default_bot` is used. This warmup might be still
      useful in case of long system prompt.

    - `:VQQWarmCtx [@bot]` will send current chat messages as a warmup query. 
      Will also include the visual selection. It @bot tag is present, `@bot`
      will be used to generate response, otherwise `g:vqq_default_bot` is used.

    - `:VQQWarmNewCtx [@bot]` will send new chat as a warmup query. 
      Will also include the visual selection. It @bot tag is present, `@bot`
      will be used to generate response, otherwise `g:vqq_default_bot` is used.

Next group of commands is UI-related.

    - `:VQQList` will show the list of past chat threads. User can navigate the
      list and select chat session by pressing <CR>. This action will make chat
      session current.

    - `:VQQOpenChat chat_id` opens chat with id=`chat_id`. This action will make
      chat session current.

    - `:VQQToggle` shows/hides vimqq window.

==============================================================================
5. Mappings                                                     *vimqq-mappings*  

vimqq adds no global key mappings, only the navigation within chat buffer.

In chat list view:
 - 'q'  will close the vim-qq window
 - <cr> will select chat session under cursor, open it and make current.

In chat view:
 - 'q' will open the chat list view while keeping the same current chat.

It is a good idea to define some for yourself, but before that we need to
look at several helper functions:

  - `VQQWarmup(bot)` will call `:VQQWarmCtx bot` and prefill command line with
    `:'<,'>VQQSendCtx bot`, so that user can start typing question immediately.

  - `VQQWarmupNew(bot)` will call `:VQQWarmNewCtx bot` and prefill command
    line with `:'<,'>VQQSendNewCtx bot`, so that user can start typing 
    question immediately.

  - `VQQQuery(bot)` will prefill command line with `:'<,'>VQQSend bot`,
    so that user can start typing question immediately.

  - `VQQQueryNew(bot)` will prefill command line with `:'<,'>VQQSendNew bot`,
    so that user can start typing question immediately.

Using these functions we can define custom key mappings to improve productivity.

Let's assume we have configured two bots: `llama70` and `sonnet`:
>
    let g:vqq_llama_servers = [
          \  {'bot_name': 'llama70', 'addr': 'http://localhost:8080'}
    \]
    let g:vqq_claude_models = [
          \  {'bot_name': 'sonnet', 'model': 'claude-3-5-sonnet-20240620'}
    \]

Now we can define some key mappings
>
    " [w]armup llama70b
    xnoremap <silent> <leader>w :<C-u>call VQQWarmup('@llama70')<cr>
    " [w]armup new chat llama70b
    xnoremap <silent> <leader>ww :<C-u>call VQQWarmupNew('@llama70')<cr>
    " [q]uery llama70b
    nnoremap <silent> <leader>q :<C-u>call VQQQuery('@llama70')<cr>
    nnoremap <silent> <leader>qq :<C-u>call VQQQueryNew('@llama70')<cr>
    " query [s]onnet without selected context
    nnoremap <silent> <leader>s :<C-u>call VQQQuery('@sonnet')<cr>
    nnoremap <silent> <leader>ss :<C-u>call VQQQueryNew('@sonnet')<cr>
    " query [s]onnet with selected context in visual mode
    xnoremap <silent> <leader>s :<C-u>call VQQWarmup('@sonnet')<cr>
    xnoremap <silent> <leader>ss :<C-u>call VQQWarmupNew('@sonnet')<cr>

For example, <leader>ww in visual mode will get the selection, send warmup
query to llama70 bot, prefill command line with :'<,'>VQQSendNewCtx @llama70 
and wait for user input to complete the query.

Note that because warmup is no-op for remote sonnet model, we can reuse the same 
`VQQWarmup` functions to capture context and prepare the right prompt.

All the commands above expect some user message. We can also prepare some key
mapping for predefined messages for commonly used patterns. For example:
>
    " [E]xplain
    xnoremap <silent> <leader>E :<C-u>execute "'<,'>VQQSendNewCtx @llama70 Explain how this code works."<cr>
    " [I]mprove
    xnoremap <silent> <leader>I :<C-u>execute "'<,'>VQQSendNewCtx @llama70 How would you improve this code?"<cr>

==============================================================================
6. Configuration                                                  *vimqq-config*  

TBD, just list everything here for now

let g:vqq_llama_servers = get(g:, 'vqq_llama_servers', [])
let g:vqq_claude_models = get(g:, 'vqq_claude_models', [])
let g:vqq_default_bot   = get(g:, 'vqq_default_bot',   '')

let g:vqq_warmup_on_chat_open = get(g:, 'vqq_warmup_on_chat_open', [])
let g:vqq_context_template = get(g:, 'vqq_context_template', 
    \ "Here's a code snippet: \n\n{vqq_ctx}\n\n{vqq_msg}")

" default chat window width
let g:vqq_width = get(g:, 'vqq_width', 80)

" format to use in chat list
let g:vqq_time_format = get(g:, 'vqq_time_format', "%b %d %H:%M ")

" chat file
let g:vqq_chats_file = get(g:, 'vqq_chats_file', expand('~/.vim/vqq_chats.json'))

==============================================================================
7. Changelog                                                   *vimqq-changelog*  

